{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOlc0eSi6ksE"
      },
      "source": [
        "# Notebook API Python Spark GraphFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbxn8n5Io_kY"
      },
      "source": [
        "**GraphFrames Quick-Start Guide:**\n",
        "*   https://graphframes.github.io/graphframes/docs/_site/quick-start.html\n",
        "\n",
        "**GraphFrames User Guide:**\n",
        "*   https://graphframes.github.io/graphframes/docs/_site/user-guide.html\n",
        "\n",
        "**GraphFrames Python API**\n",
        "*   https://graphframes.github.io/graphframes/docs/_site/api/python/index.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtf54D-Z64CH"
      },
      "source": [
        "***Install pyspark and findspark:***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioEYznLp69Sm"
      },
      "source": [
        "***Install GraphFrames :***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import igraph as ig\n",
        "import os\n",
        "# !find /usr/local -name \"pyspark\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/home/anyes/spark\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QHgUCUv7P5v"
      },
      "source": [
        "***Start the spark session:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS12q7oh7ZBg",
        "outputId": "2cf46bbe-84ae-418b-d7dc-87fb51cfe709"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/02/06 10:56:25 WARN Utils: Your hostname, anyes-Latitude-5480 resolves to a loopback address: 127.0.1.1; using 10.51.22.213 instead (on interface wlp2s0)\n",
            "24/02/06 10:56:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":: loading settings :: url = jar:file:/home/anyes/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ivy Default Cache set to: /home/anyes/.ivy2/cache\n",
            "The jars for the packages stored in: /home/anyes/.ivy2/jars\n",
            "graphframes#graphframes added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5c273579-89cb-4f9e-8940-570359c240d0;1.0\n",
            "\tconfs: [default]\n",
            "\tfound graphframes#graphframes;0.8.3-spark3.5-s_2.12 in spark-packages\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            ":: resolution report :: resolve 446ms :: artifacts dl 14ms\n",
            "\t:: modules in use:\n",
            "\tgraphframes#graphframes;0.8.3-spark3.5-s_2.12 from spark-packages in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-5c273579-89cb-4f9e-8940-570359c240d0\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 2 already retrieved (0kB/11ms)\n",
            "24/02/06 10:56:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/02/06 10:56:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "24/02/06 10:56:31 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
            "24/02/06 10:56:31 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "session started, its id is  local-1707213392001\n"
          ]
        }
      ],
      "source": [
        "# Main imports\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "\n",
        "# for dataframe and udf\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from datetime import *\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# initialise environment variables for spark\n",
        "findspark.init()\n",
        "\n",
        "# Start spark session\n",
        "# --------------------------\n",
        "def start_spark():\n",
        "  local = \"local[*]\"\n",
        "  appName = \"TP\"\n",
        "\n",
        "  gf = \"graphframes:graphframes:0.8.3-spark3.5-s_2.12\"\n",
        "\n",
        "  configLocale = SparkConf().setAppName(appName).setMaster(local).\\\n",
        "  set(\"spark.executor.memory\", \"6G\").\\\n",
        "  set(\"spark.driver.memory\",\"6G\").\\\n",
        "  set(\"spark.sql.catalogImplementation\",\"in-memory\").\\\n",
        "  set(\"spark.jars.packages\", gf)\n",
        "\n",
        "  spark = SparkSession.builder.config(conf = configLocale).getOrCreate()\n",
        "  sc = spark.sparkContext\n",
        "  sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "  spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\n",
        "\n",
        "  # Adjust the query execution environment to the size of the cluster (4 cores)\n",
        "  spark.conf.set(\"spark.sql.shuffle.partitions\",\"4\")\n",
        "  print(\"session started, its id is \", sc.applicationId)\n",
        "  return spark\n",
        "spark = start_spark()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7pVjl4xgTyo",
        "outputId": "6e64f4b0-c92d-420b-abbd-99d67da6e2d2"
      },
      "outputs": [],
      "source": [
        "#Import GraphFrames\n",
        "from graphframes import GraphFrame\n",
        "from graphframes.lib import AggregateMessages as AM\n",
        "from graphframes.lib import Pregel\n",
        "\n",
        "#For connectedComponents()\n",
        "# !pwd\n",
        "# !mkdir /content/checkpoints\n",
        "spark.sparkContext.setCheckpointDir('./content/checkpoints')\n",
        "\n",
        "#Import networkx\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture des fichiers du dossier BDLE_1M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJc4RwVTeRE2",
        "outputId": "dcfe1c6f-c09b-4b1e-86e0-b341b2276bd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------+-----+\n",
            "|     seqID1|     seqID2|  sim|\n",
            "+-----------+-----------+-----+\n",
            "|117761605:5|152890023:5| 97.8|\n",
            "|152904885:3|155591878:2| 88.5|\n",
            "|152887848:4|153682181:0|100.0|\n",
            "|152937692:5| 80009514:2| 82.3|\n",
            "|152990923:2|154549183:4| 98.0|\n",
            "|152867782:1|153171917:1| 83.1|\n",
            "| 15111981:2|153137370:1|100.0|\n",
            "|152794195:0| 15280704:2| 96.9|\n",
            "| 62963742:1| 63783418:5| 98.4|\n",
            "|152170568:3|153062631:2| 97.6|\n",
            "+-----------+-----------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialisez une session Spark\n",
        "spark = SparkSession.builder.appName(\"Proteines_graph\").getOrCreate()\n",
        "\n",
        "parquet_folder = \"./local/data/BDLE_10K\"\n",
        "\n",
        "# Récuperer tous les fichiers Parquet compressé avec Snappy\n",
        "parquet_files = parquet_folder + \"/*.snappy.parquet\"\n",
        "\n",
        "#Création du data frame\n",
        "df = spark.read.format(\"parquet\").option(\"compression\", \"snappy\").load(parquet_files)\n",
        "\n",
        "# Affichez les 10 premiére ligne du DataFrame\n",
        "df.show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de lignes : 20842\n",
            "Nombre de colonnes : 3\n",
            "Noms des colonnes : ['seqID1', 'seqID2', 'sim']\n"
          ]
        }
      ],
      "source": [
        "# Nombre de lignes\n",
        "nb_rows = df.count()\n",
        "\n",
        "# Liste des noms des colonnes\n",
        "columns_list = df.columns\n",
        "\n",
        "# Nombre de colonnes\n",
        "nb_columns = len(columns_list)\n",
        "\n",
        "# Affichage des informations\n",
        "print(f\"Nombre de lignes : {nb_rows}\")\n",
        "print(f\"Nombre de colonnes : {nb_columns}\")\n",
        "print(f\"Noms des colonnes : {columns_list}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Création d'un graphe avec Graphframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Créez un DataFrame distinct contenant tous les identifiants uniques de sommets\n",
        "vertices = df.selectExpr(\"seqID1 as id\").union(df.selectExpr(\"seqID2 as id\")).distinct()\n",
        "\n",
        "# Créez un DataFrame d'arêtes avec les colonnes \"src\", \"dst\", et \"sim\"\n",
        "edges = df.select(\"seqID1\", \"seqID2\", \"sim\").withColumnRenamed(\"seqID1\", \"src\").withColumnRenamed(\"seqID2\", \"dst\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de noeuds : 18500\n",
            "Nombre d'arêtes : 20842\n"
          ]
        }
      ],
      "source": [
        "# Créez le GraphFrame\n",
        "graph = GraphFrame(vertices, edges)\n",
        "nombre_noeuds = graph.vertices.count()\n",
        "nombre_aretes = graph.edges.count()\n",
        "\n",
        "print(\"Nombre de noeuds :\", nombre_noeuds)\n",
        "print(\"Nombre d'arêtes :\", nombre_aretes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prendre que les arêtes qui ont un pourcentage de similarité supérieure ou égale à 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de noeuds : 5908\n",
            "Nombre d'arêtes : 4054\n"
          ]
        }
      ],
      "source": [
        "graph_100_edges = graph.edges.filter(\"sim >= 100\")\n",
        "graph_100 = GraphFrame(vertices, graph_100_edges)\n",
        "graph_100 = graph_100.dropIsolatedVertices()\n",
        "\n",
        "nombre_noeuds = graph_100.vertices.count()\n",
        "nombre_aretes = graph_100.edges.count()\n",
        "\n",
        "print(\"Nombre de noeuds :\", nombre_noeuds)\n",
        "print(\"Nombre d'arêtes :\", nombre_aretes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Calcul des composantes connexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rntpXKP4LPe",
        "outputId": "30dea47e-ad3c-4ab1-c172-0d8e319cdeb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temps d'execution :  162.0920000076294\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "connected_components = graph_100.connectedComponents()\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Temps d'execution : \", end_time - start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ipQlgPS5Mq9"
      },
      "source": [
        "**Strongly connected components**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4AJwROu5PXB",
        "outputId": "2cfbb92b-503b-494a-a12e-519d6d84318c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+---+---------+\n",
            "| id|   name|age|component|\n",
            "+---+-------+---+---------+\n",
            "|  a|  Alice| 34|        0|\n",
            "|  d|  David| 29|        0|\n",
            "|  e| Esther| 32|        0|\n",
            "|  b|    Bob| 36|        1|\n",
            "|  c|Charlie| 30|        1|\n",
            "|  f|  Fanny| 36|        5|\n",
            "+---+-------+---+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "g.stronglyConnectedComponents(maxIter=10).orderBy(\"component\").show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
