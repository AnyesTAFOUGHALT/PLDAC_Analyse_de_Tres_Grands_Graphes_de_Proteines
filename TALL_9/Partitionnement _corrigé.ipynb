{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a11117-d512-4925-8aeb-026b9cf91225",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importation des bibliothéques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9943c7f-202a-4e7d-aa80-22a2560fc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86874681-b52b-4bde-98a8-d82989d3855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for dataframe and udf\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from igraph import Graph\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import collect_set, min as min_, expr,array_min,max\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from pyspark.sql.types import StringType, IntegerType, LongType, FloatType, ArrayType,StructType,StructField, BooleanType\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.types import BooleanType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97873d39-64d5-469d-8c23-bfb010ac3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"] =  \"/root/anaconda3/lib/python3.9/site-packages/pyspark\" \n",
    "os.environ[\"JAVA_HOME\"] =\"/usr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96490a68-57e2-4831-b0c5-6db2c3cc75f2",
   "metadata": {},
   "source": [
    "# Lancer spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c24f03-afa0-4047-b16a-4e5522fd4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init() \n",
    "\n",
    "def demarrer_spark(NB_CORES):\n",
    "  local = f\"local[{NB_CORES}]\"\n",
    "  # le parametre spark.local.dir indique le repertoire contenant les données temporaires ecrites sur disque lorsque le shufle ne tient pas en memoire\n",
    "  appName = \"PLDAC\"\n",
    "  configLocale = SparkConf().setAppName(appName).setMaster(local).\\\n",
    "  set(\"spark.executor.memory\", \"320G\").\\\n",
    "  set(\"spark.driver.memory\",\"320G\").\\\n",
    "  set(\"spark.sql.catalogImplementation\",\"in-memory\").\\\n",
    "  set(\"spark.driver.maxResultSize\", \"20G\").\\\n",
    "  set(\"spark.local.dir\", \"/data/bd/spark/tmp\").\\\n",
    "  set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    \n",
    "\n",
    "# # to allow sharing in memory arow format between pandas and spark : speeds up the creation of a spark df from a pandas df\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "\n",
    "  spark = SparkSession.builder.config(conf = configLocale).getOrCreate()\n",
    "  sc = spark.sparkContext\n",
    "  sc.setLogLevel(\"ERROR\")\n",
    "  \n",
    "  # spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\n",
    "    \n",
    "  # On ajuste l'environnement d'exécution des requêtes à la taille du cluster (NB_CORES coeurs)\n",
    "  shuffle_partitions = 3 * NB_CORES\n",
    "  print(\"shuffle\", shuffle_partitions)\n",
    "  spark.conf.set(\"spark.sql.shuffle.partitions\", str(shuffle_partitions))    \n",
    "\n",
    "  print(\"session démarrée, son id est \", sc.applicationId)\n",
    "  return spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b68b56c-c847-4c62-a89e-d1b422d4b8a4",
   "metadata": {},
   "source": [
    "# Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf8cd40-1ec4-4819-81ee-3905bb234be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/root/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/04/28 00:20:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/28 00:20:49 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "24/04/28 00:20:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/04/28 00:20:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle 180\n",
      "session démarrée, son id est  local-1714256450809\n"
     ]
    }
   ],
   "source": [
    "spark = demarrer_spark(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acd88b14-b72d-4b79-9454-895360e1d1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+-------------+------------+----------------+-------+------------+---+\n",
      "|   query_id|query_length|  target_id|target_length|match_length|percent_identity|e_value|relative_sim|  r|\n",
      "+-----------+------------+-----------+-------------+------------+----------------+-------+------------+---+\n",
      "|100003172:0|          89|143094166:3|          799|          89|           100.0|2.2E-46|         1.0|  1|\n",
      "|100003668:0|         102| 48815973:0|          222|         102|            99.0|3.9E-55|        0.99|  1|\n",
      "|100004301:0|         169|106189136:2|          695|         169|            99.4|3.9E-94|       0.994|  1|\n",
      "|100004301:0|         169|117840121:3|          264|         169|           100.0|1.9E-94|         1.0|  1|\n",
      "|100004301:0|         169| 44038038:1|          524|         169|           100.0|5.4E-94|         1.0|  1|\n",
      "|100004301:0|         169| 49847952:5|          458|         169|            99.4|1.2E-93|       0.994|  1|\n",
      "|100004301:0|         169| 68146322:0|          509|         169|            99.4|3.6E-94|       0.994|  1|\n",
      "|100004301:0|         169| 91784634:5|          532|         169|            99.4|2.2E-94|       0.994|  1|\n",
      "| 10000486:1|          77| 40045340:2|           84|          77|           100.0|1.1E-28|         1.0|  1|\n",
      "|100005477:3|          83|  1061792:3|          295|          83|           100.0|2.3E-42|         1.0|  1|\n",
      "|100005477:3|          83| 46123706:0|          215|          83|           100.0|6.9E-42|         1.0|  1|\n",
      "|100007146:0|          89|110429047:0|          153|          89|           100.0|1.2E-41|         1.0|  1|\n",
      "|100007146:0|          89| 12065225:3|          324|          89|           100.0|1.2E-41|         1.0|  1|\n",
      "|100007146:0|          89|125644689:2|          264|          89|           100.0|1.3E-41|         1.0|  1|\n",
      "|100007146:0|          89|125759723:0|          219|          89|           100.0|1.3E-41|         1.0|  1|\n",
      "|100007146:0|          89|136141594:0|          202|          89|           100.0|1.3E-41|         1.0|  1|\n",
      "|100007146:0|          89|139761590:0|          154|          89|           100.0|1.3E-41|         1.0|  1|\n",
      "|100007146:0|          89|152563487:0|          237|          89|           100.0|1.6E-41|         1.0|  1|\n",
      "|100007146:0|          89| 16942694:2|          249|          89|           100.0|3.7E-41|         1.0|  1|\n",
      "|100007146:0|          89| 21547212:2|          199|          89|           100.0|3.7E-41|         1.0|  1|\n",
      "+-----------+------------+-----------+-------------+------------+----------------+-------+------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_folder = \"/data/bd/dataset/proteine/80_80/G99/graph_deduplicated_v2/\"\n",
    "\n",
    "df = spark.read.parquet(parquet_folder)\n",
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bed269db-8e5f-4a96-a2e1-054484525e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341807741"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['target_id', 'query_id']]\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86113ec5-8b16-4549-bbf3-4a9890b1956e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Répartissez le DataFrame en n partitions\n",
    "nb_partitions = [20,70,100,150]\n",
    "for num_partitions in nb_partitions :\n",
    "    df_repartitioned = df.repartition(num_partitions, 'target_id')\n",
    "\n",
    "    # Écrivez le DataFrame réparti sur le disque\n",
    "    # Vous pouvez ajuster le format et l'emplacement de sortie selon vos besoins\n",
    "    output_path = f\"/data/bd/dataset/proteine/80_80/G99/TEST_RACHA_CALCUL_COMPOSANTES/partitions/{num_partitions}_partitions\"\n",
    "    df_repartitioned.write.mode(\"overwrite\").parquet(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b249095-186b-4cb9-804d-97d4e2c0a9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier /data/bd/dataset/proteine/80_80/G99/TEST_RACHA_CALCUL_COMPOSANTES/partitions/50_partitions/part-00049-bbae4cde-d0d9-4958-9a0a-5953aa180d0c-c000.snappy.parquet contient 6904102 lignes.\n"
     ]
    }
   ],
   "source": [
    "num_partitions = 50\n",
    "output_path = f\"/data/bd/dataset/proteine/80_80/G99/TEST_RACHA_CALCUL_COMPOSANTES/partitions/{num_partitions}_partitions/part-00049-bbae4cde-d0d9-4958-9a0a-5953aa180d0c-c000.snappy.parquet\"\n",
    "\n",
    "df = spark.read.parquet(output_path)\n",
    "num_rows = df.count()\n",
    "print(f\"Le fichier {output_path} contient {num_rows} lignes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec9a6712-b750-4d1d-b037-85505701a8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier /data/bd/dataset/proteine/80_80/G99/TEST_RACHA_CALCUL_COMPOSANTES/partitions/150_partitions/part-00149-cf23f7e6-df01-44ab-a44f-c9faa14d4ab4-c000.snappy.parquet contient 2305246 lignes.\n"
     ]
    }
   ],
   "source": [
    "num_partitions = 150\n",
    "output_path = f\"/data/bd/dataset/proteine/80_80/G99/TEST_RACHA_CALCUL_COMPOSANTES/partitions/{num_partitions}_partitions/part-00149-cf23f7e6-df01-44ab-a44f-c9faa14d4ab4-c000.snappy.parquet\"\n",
    "\n",
    "df = spark.read.parquet(output_path)\n",
    "num_rows = df.count()\n",
    "print(f\"Le fichier {output_path} contient {num_rows} lignes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd25247-85af-4e3c-845e-f03650605fcc",
   "metadata": {},
   "source": [
    "# Partionnement des arêtes en plusieurs sous ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d94995c-dca2-457b-9515-1697904c1ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_id</th>\n",
       "      <th>query_id</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EukProt-v2_GFUD01036155.1.p1</td>\n",
       "      <td>100000481:3</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12062594:2</td>\n",
       "      <td>100003668:0</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56659524:1</td>\n",
       "      <td>100003668:0</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136300595:3</td>\n",
       "      <td>100004174:2</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142753793:3</td>\n",
       "      <td>100004301:0</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341807736</th>\n",
       "      <td>METdb_00414-1-Transcript-29652.p1</td>\n",
       "      <td>TRINITY-DN9947-c4-g3-i1.p1</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341807737</th>\n",
       "      <td>EukProt-v2_CAMPEP_0184944524</td>\n",
       "      <td>TRINITY-DN9969-c0-g4-i1.p1</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341807738</th>\n",
       "      <td>METdb_00414-1-Transcript-29735.p1</td>\n",
       "      <td>TRINITY-DN9976-c0-g1-i3.p1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341807739</th>\n",
       "      <td>141777669:5</td>\n",
       "      <td>TRINITY-DN9989-c4-g4-i1.p1</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341807740</th>\n",
       "      <td>142407845:3</td>\n",
       "      <td>TRINITY-DN9991-c0-g1-i10.p1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>341807741 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   target_id                     query_id  \\\n",
       "0               EukProt-v2_GFUD01036155.1.p1                  100000481:3   \n",
       "1                                 12062594:2                  100003668:0   \n",
       "2                                 56659524:1                  100003668:0   \n",
       "3                                136300595:3                  100004174:2   \n",
       "4                                142753793:3                  100004301:0   \n",
       "...                                      ...                          ...   \n",
       "341807736  METdb_00414-1-Transcript-29652.p1   TRINITY-DN9947-c4-g3-i1.p1   \n",
       "341807737       EukProt-v2_CAMPEP_0184944524   TRINITY-DN9969-c0-g4-i1.p1   \n",
       "341807738  METdb_00414-1-Transcript-29735.p1   TRINITY-DN9976-c0-g1-i3.p1   \n",
       "341807739                        141777669:5   TRINITY-DN9989-c4-g4-i1.p1   \n",
       "341807740                        142407845:3  TRINITY-DN9991-c0-g1-i10.p1   \n",
       "\n",
       "           partition  \n",
       "0                111  \n",
       "1                187  \n",
       "2                163  \n",
       "3                142  \n",
       "4                153  \n",
       "...              ...  \n",
       "341807736        174  \n",
       "341807737         70  \n",
       "341807738         60  \n",
       "341807739        141  \n",
       "341807740         17  \n",
       "\n",
       "[341807741 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Nombre de partitions\n",
    "# num_partitions = 200\n",
    "\n",
    "# # Ajout d'une nouvelle colonne 'partition' basée sur le modulo de seqID1\n",
    "# df['partition'] = df['target_id'].apply(lambda x: hash(x) % num_partitions)\n",
    "\n",
    "# # Afficher le DataFrame partitionné\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31b27512-58ce-441b-99e1-5eb988d23f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_freq = df.groupby('partition').size().reset_index(name='count')\n",
    "partitions = partition_freq['partition'].unique()\n",
    "for partition in partitions:\n",
    "    df_partition = df[df['partition'] == partition]\n",
    "    file_path = f\"/data/bd/dataset/proteine/80_80/G99/TEST_RACHA_CALCUL_COMPOSANTES/200_partitions/partition_{partition}.parquet\"\n",
    "    df_partition.to_parquet(file_path, index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3b97c60-565f-40ae-8b57-c17aba842eed",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`partition`' given input columns: [query_id, target_id];\n'Aggregate ['partition], ['partition, count(1) AS count#77L]\n+- Project [target_id#2, query_id#0]\n   +- Relation[query_id#0,query_length#1L,target_id#2,target_length#3L,match_length#4L,percent_identity#5,e_value#6,relative_sim#7,r#8] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_57551/1673713769.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpartition_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'partition'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpartition_freq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`partition`' given input columns: [query_id, target_id];\n'Aggregate ['partition], ['partition, count(1) AS count#77L]\n+- Project [target_id#2, query_id#0]\n   +- Relation[query_id#0,query_length#1L,target_id#2,target_length#3L,match_length#4L,percent_identity#5,e_value#6,relative_sim#7,r#8] parquet\n"
     ]
    }
   ],
   "source": [
    "partition_freq = df.groupby('partition').sizecount(à\n",
    "partition_freq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
