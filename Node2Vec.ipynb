{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des bibliothéques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anyes/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import string\n",
    "from node2vec import Node2Vec\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche dela composante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# !find /usr/local -name \"pyspark\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/anyes/spark\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/23 12:53:01 WARN Utils: Your hostname, anyes-Latitude-5480 resolves to a loopback address: 127.0.1.1; using 10.51.39.215 instead (on interface wlp2s0)\n",
      "24/04/23 12:53:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/anyes/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/anyes/.ivy2/cache\n",
      "The jars for the packages stored in: /home/anyes/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c892a0e2-1259-4279-b992-8528f43b37a4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.3-spark3.5-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 446ms :: artifacts dl 24ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.3-spark3.5-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c892a0e2-1259-4279-b992-8528f43b37a4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/36ms)\n",
      "24/04/23 12:53:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session started, its id is  local-1713869588591\n"
     ]
    }
   ],
   "source": [
    "# Main imports\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for dataframe and udf\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# initialise environment variables for spark\n",
    "findspark.init()\n",
    "\n",
    "# Start spark session\n",
    "# --------------------------\n",
    "def start_spark():\n",
    "  local = \"local[*]\"\n",
    "  appName = \"PLDAC\"\n",
    "\n",
    "  gf = \"graphframes:graphframes:0.8.3-spark3.5-s_2.12\"\n",
    "\n",
    "  configLocale = SparkConf().setAppName(appName).setMaster(local).\\\n",
    "  set(\"spark.executor.memory\", \"6G\").\\\n",
    "  set(\"spark.driver.memory\",\"6G\").\\\n",
    "  set(\"spark.sql.catalogImplementation\",\"in-memory\").\\\n",
    "  set(\"spark.jars.packages\", gf)\n",
    "\n",
    "  spark = SparkSession.builder.config(conf = configLocale).getOrCreate()\n",
    "  sc = spark.sparkContext\n",
    "  sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "  spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\n",
    "\n",
    "  # Adjust the query execution environment to the size of the cluster (4 cores)\n",
    "  spark.conf.set(\"spark.sql.shuffle.partitions\",\"4\")\n",
    "  print(\"session started, its id is \", sc.applicationId)\n",
    "  return spark\n",
    "spark = start_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------+\n",
      "|     seq_id|component_id|pfamAcc|\n",
      "+-----------+------------+-------+\n",
      "| 10000015:4|     5776874|   NULL|\n",
      "|100000849:0|     3399826|PF01248|\n",
      "|100001698:0|     2440451|PF00347|\n",
      "|100001985:3|     9326203|PF01576|\n",
      "|100003037:5|     2736447|   NULL|\n",
      "|100008959:5|     6897091|PF02574|\n",
      "|100009830:4|      831438|PF02820|\n",
      "|100012128:5|     6884991|   NULL|\n",
      "|100014733:3|     6142334|   NULL|\n",
      "|100020155:3|       62140|PF08033|\n",
      "+-----------+------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialisez une session Spark\n",
    "spark = SparkSession.builder.appName(\"Proteines_graph\").getOrCreate()\n",
    "\n",
    "parquet_folder = \"./local/data/node_with_comp_and_annotation\"\n",
    "\n",
    "# Récuperer tous les fichiers Parquet compressé avec Snappy\n",
    "parquet_files = parquet_folder + \"/*.snappy.parquet\"\n",
    "\n",
    "#Création du data frame\n",
    "df = spark.read.format(\"parquet\").option(\"compression\", \"snappy\").load(parquet_files)\n",
    "\n",
    "# Affichez les 10 premiére ligne du DataFrame\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 4) / 6]\r"
     ]
    }
   ],
   "source": [
    "df_grouped = df.groupBy(\"component_id\").agg(count(\"*\").alias(\"count\"))\n",
    "df_grouped.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = None #Création d'un graph avec networkx\n",
    "\n",
    "# Générer les embeddings de noeuds avec Node2Vec\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "# Obtenir les embeddings de noeuds appris\n",
    "node_embeddings = model.wv\n",
    "\n",
    "labeled_nodes = None # La liste des noeuds qui ont des labels\n",
    "one_hot_labels = None # Convertir les étiquettes en vecteurs One-Hot Encoding\n",
    "total_label_count = None # Nombre total de labels (pfam)\n",
    "\n",
    "# Créer les caractéristiques X et les étiquettes y pour la classification\n",
    "X = np.array([node_embeddings.get_vector(str(node)) for node in labeled_nodes])\n",
    "y = np.array([one_hot_labels.get(node, [0] * total_label_count) for node in labeled_nodes])\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création du classificateur MLPClassifier\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=100, activation='relu', solver='adam', random_state=42)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes pour les données de test\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculer l'exactitude\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Définition des hyperparamètres à rechercher\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'max_iter': [100, 200]\n",
    "}\n",
    "\n",
    "# Création de l'objet GridSearchCV\n",
    "grid_search = GridSearchCV(MLPClassifier(random_state=42), param_grid, cv=5)\n",
    "\n",
    "# Recherche des meilleurs hyperparamètres\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Affichage des meilleurs hyperparamètres trouvés\n",
    "print(\"Meilleurs hyperparamètres trouvés :\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Obtention du meilleur modèle\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Prédire les étiquettes pour les données de test avec le meilleur modèle\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculer l'exactitude avec le meilleur modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy avec le meilleur modèle:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
