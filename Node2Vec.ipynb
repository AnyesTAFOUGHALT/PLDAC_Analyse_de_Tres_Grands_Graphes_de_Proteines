{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des bibliothéques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import string\n",
    "from node2vec import Node2Vec\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche dela composante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# !find /usr/local -name \"pyspark\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/anyes/spark\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/23 23:58:43 WARN Utils: Your hostname, anyes-Latitude-5480 resolves to a loopback address: 127.0.1.1; using 192.168.1.45 instead (on interface wlp2s0)\n",
      "24/04/23 23:58:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/anyes/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/anyes/.ivy2/cache\n",
      "The jars for the packages stored in: /home/anyes/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0d6a75ef-1254-4790-8676-444e36026062;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.3-spark3.5-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 787ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.3-spark3.5-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0d6a75ef-1254-4790-8676-444e36026062\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/16ms)\n",
      "24/04/23 23:58:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session started, its id is  local-1713909531059\n"
     ]
    }
   ],
   "source": [
    "# Main imports\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for dataframe and udf\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# initialise environment variables for spark\n",
    "findspark.init()\n",
    "\n",
    "# Start spark session\n",
    "# --------------------------\n",
    "def start_spark():\n",
    "  local = \"local[*]\"\n",
    "  appName = \"PLDAC\"\n",
    "\n",
    "  gf = \"graphframes:graphframes:0.8.3-spark3.5-s_2.12\"\n",
    "\n",
    "  configLocale = SparkConf().setAppName(appName).setMaster(local).\\\n",
    "  set(\"spark.executor.memory\", \"6G\").\\\n",
    "  set(\"spark.driver.memory\",\"6G\").\\\n",
    "  set(\"spark.sql.catalogImplementation\",\"in-memory\").\\\n",
    "  set(\"spark.jars.packages\", gf)\n",
    "\n",
    "  spark = SparkSession.builder.config(conf = configLocale).getOrCreate()\n",
    "  sc = spark.sparkContext\n",
    "  sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "  spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\n",
    "\n",
    "  # Adjust the query execution environment to the size of the cluster (4 cores)\n",
    "  spark.conf.set(\"spark.sql.shuffle.partitions\",\"4\")\n",
    "  print(\"session started, its id is \", sc.applicationId)\n",
    "  return spark\n",
    "spark = start_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Initialisez une session Spark\n",
    "spark = SparkSession.builder.appName(\"Proteines_graph\").getOrCreate()\n",
    "\n",
    "parquet_folder = \"./local/data/component_9/graph\"\n",
    "\n",
    "# Récuperer tous les fichiers Parquet compressé avec Snappy\n",
    "parquet_files = parquet_folder + \"/*.snappy.parquet\"\n",
    "\n",
    "#Création du data frame\n",
    "df = spark.read.format(\"parquet\").option(\"compression\", \"snappy\").load(parquet_files)\n",
    "\n",
    "# Affichez les 10 premiére ligne du DataFrame\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 4) / 6]\r"
     ]
    }
   ],
   "source": [
    "df_grouped = df.groupBy(\"component_id\").agg(count(\"*\").alias(\"count\"))\n",
    "df_grouped.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = None #Création d'un graph avec networkx\n",
    "\n",
    "# Générer les embeddings de noeuds avec Node2Vec\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "# Obtenir les embeddings de noeuds appris\n",
    "node_embeddings = model.wv\n",
    "\n",
    "labeled_nodes = None # La liste des noeuds qui ont des labels\n",
    "one_hot_labels = None # Convertir les étiquettes en vecteurs One-Hot Encoding\n",
    "total_label_count = None # Nombre total de labels (pfam)\n",
    "\n",
    "# Créer les caractéristiques X et les étiquettes y pour la classification\n",
    "X = np.array([node_embeddings.get_vector(str(node)) for node in labeled_nodes])\n",
    "y = np.array([one_hot_labels.get(node, [0] * total_label_count) for node in labeled_nodes])\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création du classificateur MLPClassifier\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=100, activation='relu', solver='adam', random_state=42)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes pour les données de test\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculer l'exactitude\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Définition des hyperparamètres à rechercher\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'max_iter': [100, 200]\n",
    "}\n",
    "\n",
    "# Création de l'objet GridSearchCV\n",
    "grid_search = GridSearchCV(MLPClassifier(random_state=42), param_grid, cv=5)\n",
    "\n",
    "# Recherche des meilleurs hyperparamètres\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Affichage des meilleurs hyperparamètres trouvés\n",
    "print(\"Meilleurs hyperparamètres trouvés :\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Obtention du meilleur modèle\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Prédire les étiquettes pour les données de test avec le meilleur modèle\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculer l'exactitude avec le meilleur modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy avec le meilleur modèle:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
